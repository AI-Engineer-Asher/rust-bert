// Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.
// Copyright 2019-2020 Guillaume Becquin
// Copyright 2020 Maarten van Gompel
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//     http://www.apache.org/licenses/LICENSE-2.0
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

//! # Token classification pipeline (Named Entity Recognition, Part-of-Speech tagging)
//! More generic token classification pipeline, works with multiple models (Bert, Roberta)
//!
//! ```no_run
//! use rust_bert::pipelines::token_classification::{ModelType,TokenClassificationModel,TokenClassificationConfig};
//! use rust_bert::resources::{Resource,RemoteResource};
//!# fn main() -> failure::Fallible<()> {
//!
//! //Load a configuration
//! let config = TokenClassificationConfig::new(ModelType::Bert,
//!    Resource::Remote(RemoteResource::from_pretrained( ("bert-large-cased-finetuned-conll03-english/rust_model.ot","https://cdn.huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/rust_model.ot" ) )),
//!    Resource::Remote(RemoteResource::from_pretrained( ("bert-large-cased-finetuned-conll03-english/config.json","https://cdn.huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/config.json" ) )),
//!    Resource::Remote(RemoteResource::from_pretrained( ("bert-large-cased-finetuned-conll03-english/vocab.txt","https://cdn.huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/vocab.txt" ) )),
//!    None, //merges resource only relevant with ModelType::Roberta
//!    false, //lowercase
//! );
//!
//! //Create the model
//! let ner_model = TokenClassificationModel::new(config)?;
//!
//! let input = [
//!     "My name is Amy. I live in Paris.",
//!     "Paris is a city in France."
//! ];
//! let output = ner_model.predict(&input, true); //ignore_first_label = true (only returns the NER parts, ignoring first label O)
//!# Ok(())
//!# }
//! ```
//! Output: \
//! ```no_run
//!# use rust_bert::pipelines::token_classification::Token;
//!# let output =
//! [
//!    Token { text: String::from("Amy"), score: 0.9986, label: String::from("I-PER"), sentence: 0, index: 0, word_index: Some(3), continuation: false },
//!    Token { text: String::from("Paris"), score: 0.9985, label: String::from("I-LOC"), sentence: 0, index: 9 , word_index: Some(8), continuation: false},
//!    Token { text: String::from("Paris"), score: 0.9988, label: String::from("I-LOC"), sentence: 1, index: 1, word_index: Some(0), continuation: false},
//!    Token { text: String::from("France"), score: 0.9993, label: String::from("I-LOC"), sentence: 1, index: 6, word_index: Some(5), continuation: false},
//! ]
//!# ;
//! ```

use rust_tokenizers::bert_tokenizer::{BertTokenizer};
use rust_tokenizers::{RobertaTokenizer, RobertaVocab, BertVocab};
use tch::nn::VarStore;
use rust_tokenizers::preprocessing::tokenizer::base_tokenizer::{Tokenizer,TokenizedInput,TruncationStrategy};
use std::path::Path;
use std::collections::HashMap;
use tch::{Tensor, no_grad, Device};
use tch::kind::Kind::Float;
use crate::bert::{BertForTokenClassification, BertConfig, BertModelResources, BertConfigResources, BertVocabResources};
use crate::roberta::{RobertaForTokenClassification};
use crate::distilbert::{DistilBertForTokenClassification, DistilBertConfig};
use crate::Config;
use crate::common::resources::{Resource, RemoteResource, download_resource};
use serde::{Serialize,Deserialize};


#[derive(Debug,Serialize,Deserialize)]
/// # Token generated by a `TokenClassificationModel`
pub struct Token {
    /// String representation of the Token
    pub text: String,

    /// Confidence score
    pub score: f64,

    /// Token label (e.g. ORG, LOC in case of NER)
    pub label: String,

    /// Sentence index
    #[serde(default)]
    pub sentence: usize,

    /// Token position index
    pub index: u16,

    /// Word index, relative to the sentence index
    pub word_index: Option<u8>,

    /// Continuation marker: marks this token as a continuation of the previous one
    #[serde(default)]
    pub continuation: bool,
}

/// # Configuration for TokenClassificationModel
/// Contains information regarding the model to load and device to place the model on.
pub struct TokenClassificationConfig {
    /// Model type
    pub model_type: ModelType,
    /// Model weights resource (default: pretrained BERT model on CoNLL)
    pub model_resource: Resource,
    /// Config resource (default: pretrained BERT model on CoNLL)
    pub config_resource: Resource,
    /// Vocab resource (default: pretrained BERT model on CoNLL)
    pub vocab_resource: Resource,
    /// Merges resource (default: pretrained BERT model on CoNLL)
    pub merges_resource: Option<Resource>,
    /// Automatically lower case all input upon tokenisation (assumes a lower-cased model)
    pub lower_case: bool,
    /// Device to place the model on (default: CUDA/GPU when available)
    pub device: Device,
}

impl TokenClassificationConfig {
    /// Instantiate a new token classification configuration of the supplied type.
    ///
    /// # Arguments
    ///
    /// * `model_type` - `ModelType` indicating the model type to load (must match with the actual data to be loaded!)
    /// * model - The `Resource` pointing to the model to load (e.g.  model.ot)
    /// * config - The `Resource' pointing to the model configuration to load (e.g. config.json)
    /// * vocab - The `Resource' pointing to the tokenizer's vocabulary to load (e.g.  vocab.txt/vocab.json)
    /// * vocab - An optional `Resource` tuple (`Option<Resource>`) pointing to the tokenizer's merge file to load (e.g.  merges.txt), needed only for Roberta.
    /// * lower_case - A `bool' indicating whether the tokeniser should lower case all input (in case of a lower-cased model)
    ///
    pub fn new(model_type: ModelType, model: Resource, config: Resource, vocab: Resource, merges: Option<Resource>, lower_case: bool) -> TokenClassificationConfig {
        TokenClassificationConfig {
            model_type: model_type,
            model_resource: model,
            config_resource: config,
            vocab_resource: vocab,
            merges_resource: merges,
            lower_case: lower_case,
            device: Device::cuda_if_available(),
        }
    }
}

impl Default for TokenClassificationConfig {
    /// Provides a default CONLL-2003 NER model (English)
    fn default() -> TokenClassificationConfig {
        TokenClassificationConfig {
            model_type: ModelType::Bert,
            model_resource: Resource::Remote(RemoteResource::from_pretrained(BertModelResources::BERT_NER)),
            config_resource: Resource::Remote(RemoteResource::from_pretrained(BertConfigResources::BERT_NER)),
            vocab_resource: Resource::Remote(RemoteResource::from_pretrained(BertVocabResources::BERT_NER)),
            merges_resource: None,
            lower_case: false,
            device: Device::cuda_if_available(),
        }
    }
}

#[derive(Clone,Copy,Serialize,Deserialize)]
/// # Identifies the type of model
pub enum ModelType {
    Bert,
    DistilBert,
    Roberta,
}

/// # Abstraction that holds a model configuration, can be of any of the supported models
pub enum ConfigOption {
    /// Bert configuration
    Bert(BertConfig),
    /// DistilBert configuration
    DistilBert(DistilBertConfig),
}

/// # Abstraction that holds a particular tokenizer, can be of any of the supported models
pub enum TokenizerOption {
    /// Bert Tokenizer
    Bert(BertTokenizer),
    /// Roberta Tokenizer
    Roberta(RobertaTokenizer),
}

/// # Abstraction that holds one particular token sequence classifier model, for any of the supported models
pub enum TokenClassificationOption {
    /// Bert for Token Classification
    Bert(BertForTokenClassification),
    /// DistilBert for Token Classification
    DistilBert(DistilBertForTokenClassification),
    /// Roberta for Token Classification
    Roberta(RobertaForTokenClassification),
}

impl ConfigOption {
    /// Interface method to load a configuration from file
    pub fn from_file(model_type: ModelType, path: &Path) -> Self {
        match model_type {
            ModelType::Bert | ModelType::Roberta => ConfigOption::Bert(BertConfig::from_file(path)),
            ModelType::DistilBert => ConfigOption::DistilBert(DistilBertConfig::from_file(path))
        }
    }

    pub fn get_label_mapping(self) -> HashMap<i64,String> {
        match self {
            Self::Bert(config) => config.id2label.expect("No label dictionary (id2label) provided in configuration file"),
            Self::DistilBert(config) => config.id2label.expect("No label dictionary (id2label) provided in configuration file"),
        }
    }
}

impl TokenizerOption {
    /// Interface method to load a tokenizer from file
    pub fn from_file(model_type: ModelType, vocab_path: &str, merges_path: Option<&str>, lower_case: bool) -> Self {
        match model_type {
            ModelType::Bert | ModelType::DistilBert => TokenizerOption::Bert(BertTokenizer::from_file(vocab_path,lower_case)),
            ModelType::Roberta => TokenizerOption::Roberta(RobertaTokenizer::from_file(vocab_path, merges_path.expect("No merges specified!"), lower_case)),
        }
    }

    /// Returns the model type
    pub fn model_type(&self) -> ModelType {
        match *self {
            Self::Bert(_) => ModelType::Bert,
            Self::Roberta(_) => ModelType::Roberta
        }
    }

    /// Interface method
    pub fn encode_list(&self, text_list: Vec<&str>, max_len: usize, truncation_strategy: &TruncationStrategy, stride: usize) -> Vec<TokenizedInput> {
        match *self {
            Self::Bert(ref tokenizer) => tokenizer.encode_list(text_list, max_len, truncation_strategy, stride),
            Self::Roberta(ref tokenizer) => tokenizer.encode_list(text_list, max_len, truncation_strategy, stride)
        }
    }
}



impl TokenClassificationOption {
    /// Instantiate a new token sequence classification model of the supplied type.
    ///
    /// # Arguments
    ///
    /// * `model_type` - `ModelType` indicating the model type to load (must match with the actual data to be loaded)
    /// * `p` - `tch::nn::Path` path to the model file to load (e.g. model.ot)
    /// * `config` - A configuration (the model type of the configuration must be compatible with the value for
    /// `model_type`)
    ///
    pub fn new(model_type: ModelType, p: &tch::nn::Path, config: &ConfigOption) -> Self {
        match model_type {
            ModelType::Bert => {
                if let ConfigOption::Bert(config) = config {
                    TokenClassificationOption::Bert(BertForTokenClassification::new(p, config))
                } else {
                    panic!("You can only supply a BertConfig for Bert!");
                }
            },
            ModelType::DistilBert => {
                if let ConfigOption::DistilBert(config) = config {
                    TokenClassificationOption::DistilBert(DistilBertForTokenClassification::new(p, config))
                } else {
                    panic!("You can only supply a DistilBertConfig for DistilBert!");
                }
            }
            ModelType::Roberta => {
                if let ConfigOption::Bert(config) = config {
                    TokenClassificationOption::Roberta(RobertaForTokenClassification::new(p, config))
                } else {
                    panic!("You can only supply a BertConfig for Roberta!");
                }
            }
        }
    }

    /// Returns the `ModelType` for this TokenClassificationOption
    pub fn model_type(&self) -> ModelType {
        match *self {
            Self::Bert(_) => ModelType::Bert,
            Self::Roberta(_) => ModelType::Roberta,
            Self::DistilBert(_) => ModelType::DistilBert
        }
    }

    /// Interface method to forward_t() of the particular models.
    pub fn forward_t(&self,
                     input_ids: Option<Tensor>,
                     mask: Option<Tensor>,
                     token_type_ids: Option<Tensor>,
                     position_ids: Option<Tensor>,
                     input_embeds: Option<Tensor>,
                     train: bool) -> (Tensor, Option<Vec<Tensor>>, Option<Vec<Tensor>>)  {
        match *self {
            Self::Bert(ref model) => model.forward_t(input_ids,mask,token_type_ids,position_ids,input_embeds,train),
            Self::DistilBert(ref model) => model.forward_t(input_ids,mask,input_embeds,train).expect("Error in distilbert forward_t"),
            Self::Roberta(ref model) => model.forward_t(input_ids,mask,token_type_ids,position_ids,input_embeds,train),
        }

    }

}


/// # TokenClassificationModel for Named Entity Recognition or Part-of-Speech tagging
pub struct TokenClassificationModel {
    tokenizer: TokenizerOption,
    token_sequence_classifier: TokenClassificationOption, //e.g. BertForTokenClassification,
    label_mapping: HashMap<i64,String>,
    var_store: VarStore,
}

impl TokenClassificationModel {
    /// Build a new `TokenClassificationModel`
    ///
    /// # Arguments
    ///
    /// * `config` - `TokenClassificationConfig` object containing the resource references (model, vocabulary, configuration) and device placement (CPU/GPU)
    ///
    /// # Example
    ///
    /// ```no_run
    ///# fn main() -> failure::Fallible<()> {
    /// use rust_bert::pipelines::token_classification::TokenClassificationModel;
    ///
    /// let model = TokenClassificationModel::new(Default::default())?;
    ///# Ok(())
    ///# }
    /// ```
    ///
    pub fn new(config: TokenClassificationConfig) -> failure::Fallible<TokenClassificationModel> {
        let config_path = download_resource(&config.config_resource)?;
        let vocab_path = download_resource(&config.vocab_resource)?;
        let weights_path = download_resource(&config.model_resource)?;
        let merges_path = if let Some(merges_resource) = &config.merges_resource {
            Some(download_resource(merges_resource).expect("Failure downloading resource"))
        } else {
            None
        };
        let device = config.device;

        let tokenizer = TokenizerOption::from_file(config.model_type, vocab_path.to_str().unwrap(), merges_path.map(|path| path.to_str().unwrap()), config.lower_case);
        let mut var_store = VarStore::new(device);
        let model_config = ConfigOption::from_file(config.model_type, config_path);
        let token_sequence_classifier = TokenClassificationOption::new(config.model_type, &var_store.root(), &model_config);
        let label_mapping = model_config.get_label_mapping();
        var_store.load(weights_path)?;
        Ok(TokenClassificationModel { tokenizer, token_sequence_classifier, label_mapping, var_store })
    }

    fn prepare_for_model(&self, input: Vec<&str>) -> Tensor {
        let tokenized_input: Vec<TokenizedInput> = self.tokenizer.encode_list(input.to_vec(),
                                                         128,
                                                         &TruncationStrategy::LongestFirst,
                                                         0);
        let max_len = tokenized_input.iter().map(|input| input.token_ids.len()).max().unwrap();
        let tokenized_input_tensors: Vec<tch::Tensor> = tokenized_input.
            iter().
            map(|input| input.token_ids.clone()).
            map(|mut input| {
                input.extend(vec![0; max_len - input.len()]);
                input
            }).
            map(|input|
                Tensor::of_slice(&(input))).
            collect::<Vec<_>>();
        Tensor::stack(tokenized_input_tensors.as_slice(), 0).to(self.var_store.device())
    }

    /// Extract entities from a text
    ///
    /// # Arguments
    ///
    /// * `input` - `&[&str]` Array of texts to extract entities from.
    ///
    /// # Returns
    ///
    /// * `Vec<Entity>` containing extracted entities
    ///
    /// # Example
    ///
    /// ```no_run
    ///# fn main() -> failure::Fallible<()> {
    ///# use rust_bert::pipelines::token_classification::TokenClassificationModel;
    ///
    /// let ner_model =  TokenClassificationModel::new(Default::default())?;
    /// let input = [
    ///     "My name is Amy. I live in Paris.",
    ///     "Paris is a city in France."
    /// ];
    /// let output = ner_model.predict(&input, true);
    ///# Ok(())
    ///# }
    /// ```
    pub fn predict(&self, input: &[&str], ignore_first_label: bool) -> Vec<Token> {
        let input_tensor = self.prepare_for_model(input.to_vec());
        let (output, _, _) = no_grad(|| {
            self.token_sequence_classifier
                .forward_t(Some(input_tensor.copy()),
                           None,
                           None,
                           None,
                           None,
                           false)
        });
        let output = output.detach().to(Device::Cpu);
        let score: Tensor = output.exp() / output.exp().sum1(&[-1], true, Float);
        let labels_idx = &score.argmax(-1, true);

        let mut tokens: Vec<Token> = vec!();
        for sentence_idx in 0..labels_idx.size()[0] {
            let labels = labels_idx.get(sentence_idx);
            let mut word_idx: u8 = 0;
            for position_idx in 0..labels.size()[0] {
                let label_id = labels.int64_value(&[position_idx]);
                let token = {
                    let token_id = input_tensor.int64_value(&[sentence_idx, position_idx]);
                    self.decode_token(token_id, label_id, &score, sentence_idx, position_idx, &mut word_idx)
                };
                if let Some(token) = token {
                    if !ignore_first_label || label_id != 0 {
                        tokens.push(token);
                    }
                }
            }
        }
        tokens
    }

    fn decode_token(&self, token_id: i64, label_id: i64, score: &Tensor, sentence_idx: i64, position_idx: i64, word_idx: &mut u8) -> Option<Token> {
        let mut text = match self.tokenizer {
            TokenizerOption::Bert(ref tokenizer) => Tokenizer::decode(tokenizer, vec!(token_id), false, false),
            TokenizerOption::Roberta(ref tokenizer) => Tokenizer::decode(tokenizer, vec!(token_id), false, false),
        };
        let special_value: bool = match self.tokenizer {
            //note: we don't count unk as a special value here, as we still want those in the output
            TokenizerOption::Bert(_) => {
                text == BertVocab::sep_value() ||
                text == BertVocab::mask_value() ||
                text == BertVocab::pad_value() ||
                text == BertVocab::cls_value()
            },
            TokenizerOption::Roberta(_) => {
                text == RobertaVocab::bos_value() ||
                text == RobertaVocab::eos_value() ||
                text == RobertaVocab::sep_value() ||
                text == RobertaVocab::mask_value() ||
                text == RobertaVocab::pad_value() ||
                text == RobertaVocab::cls_value()
            }
        };
        let continuation = !special_value && match self.tokenizer {
            TokenizerOption::Bert(_) => if text.starts_with("##") && text.len() > 2 {
                text.drain(..2); //remove the continuation tokens from the text
                true
            } else {
                false
            },
            TokenizerOption::Roberta(_) => if text.starts_with(" ") {
                text.drain(..1); //remove the leading space from the text
                false
            } else {
                match text.find(|c:  char| { !c.is_alphabetic() }) {
                    Some(0) => false,
                    _ => true,
                }
            }
        };
        if !continuation && !special_value && !text.is_empty() {
            *word_idx += 1;
        }

        if special_value {
            None
        } else {
            Some(Token {
                text: text,
                score: score.double_value(&[sentence_idx, position_idx, label_id]),
                label: self.label_mapping.get(&label_id).expect("Index out of vocabulary bounds.").to_owned(),
                sentence: sentence_idx as usize,
                index: position_idx as u16,
                word_index: Some(*word_idx - 1), //0 indexed
                continuation: continuation,
            })
        }
    }
}
